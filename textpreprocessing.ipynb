{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843ef5e8-84a6-4e8d-9b75-a9e3252b3f24",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "1. Tokenization\n",
    "* using nltk and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed804a22-7a35-4fdd-afc3-752550be4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a23474b-b944-4a7e-9d02-7d3e7521c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'you', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love you !\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024c8bdf-ac69-421e-93eb-801dbdb508c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy\n",
      "is\n",
      "a\n",
      "powerful\n",
      "NLP\n",
      "library\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Spacy is a powerful NLP library\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f45b7-81b2-43bf-9e90-294cd881184b",
   "metadata": {},
   "source": [
    "## NLTK(Natural Language tool kit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49054f28-6d98-41b1-88b8-91f2837703cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', \"'re\", 'testing', 'NLTK', \"'s\", 'tokenization', '.']\n",
      "['We', \"'re\", 'testing', 'NLTK', \"'s\", 'tokenization', '.']\n",
      "['We', \"'\", 're', 'testing', 'NLTK', \"'\", 's', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.tokenize.moses import MosesTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "text = \"We're testing NLTK's tokenization.\"\n",
    "token = nltk.word_tokenize(text)\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer1 = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token1 = tokenizer1.tokenize(text)\n",
    "print(token)\n",
    "print(tokens)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92b964be-c149-486a-8cd4-9d57439fc9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPunctTokenizer: ['Hey', '@', 'user', '!', 'Check', 'this', 'out', ':', 'https', '://', 'example', '.', 'com', 'ðŸ˜‚', '#', 'fun']\n",
      "Treebankword: ['Hey', '@', 'user', '!', 'Check', 'this', 'out', ':', 'https', ':', '//example.com', 'ðŸ˜‚', '#', 'fun']\n",
      "WhitespaceTokenizer:['Hey', '@user!', 'Check', 'this', 'out:', 'https://example.com', 'ðŸ˜‚', '#fun']\n",
      "RegexpTokenzier:['Hey', 'user', 'Check', 'this', 'out', 'https', 'example', 'com', 'fun']\n",
      "TwitterTokenzier:['Hey', '@user', '!', 'Check', 'this', 'out', ':', 'https://example.com', 'ðŸ˜‚', '#fun']\n"
     ]
    }
   ],
   "source": [
    "# types of tokenizer\n",
    "def types_of_tokenizer(tokenizer):\n",
    "    text = \"Hey @user! Check this out: https://example.com ðŸ˜‚ #fun\"\n",
    "    token = tokenizer.tokenize(text)\n",
    "    return token\n",
    "print(f\"WordPunctTokenizer: {types_of_tokenizer(WordPunctTokenizer())}\")\n",
    "print(f\"Treebankword: {types_of_tokenizer(TreebankWordTokenizer())}\")\n",
    "print(f\"WhitespaceTokenizer:{types_of_tokenizer(WhitespaceTokenizer())}\")\n",
    "print(f\"RegexpTokenzier:{types_of_tokenizer(RegexpTokenizer(r'\\w+'))}\")\n",
    "print(f\"TwitterTokenzier:{types_of_tokenizer(TweetTokenizer())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c073d6-9ff4-40b2-aed8-67f10437993c",
   "metadata": {},
   "source": [
    "### Tokenization in spaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "038f9e19-8dc2-4d1e-8daa-0e80fd8a5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'How', \"'s\", 'your', 'NLP', 'journey', 'going', '?']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Hello! How's your NLP journey going?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf01ce1-1fe6-48ea-961e-b09867648044",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "380e4d56-fede-4898-bcfb-6e01e3dc82e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['This', 'is', 'a', 'simple', 'example', 'to', 'demonstrate', 'stopword', 'removal', '.']\n",
      "Words after removing stopwords: ['simple', 'example', 'demonstrate', 'stopword', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence = \"This is a simple example to demonstrate stopword removal.\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in words:\n",
    "    if word.lower() not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Words after removing stopwords:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a54c2-4c2e-42ca-b613-d1a29743e9c6",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Types of Stemmer\n",
    "* PorterStemmer\n",
    "* LancasterStemmer(More aggressive)\n",
    "* SnowballStemmer\n",
    "* RegexpStemmer\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d16e6fc-c05a-436b-aec6-d335e2fcc7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'file', 'easili', 'fairli', 'stdui']\n"
     ]
    }
   ],
   "source": [
    "# Porterstemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = [\"running\", \"files\", \"easily\", \"fairly\", \"stduies\"]\n",
    "\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21471c1b-c3df-41de-b413-a264d18d7f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fil', 'easy', 'fair', 'stduies', '@']\n"
     ]
    }
   ],
   "source": [
    "# LancasterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "words1 = [\"running\", \"files\", \"easily\", \"fairly\", \"stduies\",\"@\"]\n",
    "ls = LancasterStemmer()\n",
    "stemmed_word1 = [ls.stem(word) for word in words1]\n",
    "print(stemmed_word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3607788-21b6-467d-bb14-b8fd92a6d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'flie', 'easi', 'fair', 'studie']\n"
     ]
    }
   ],
   "source": [
    "# RegexpStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "regexp_stemmer = RegexpStemmer('ning$|s$|ly$',min=4)\n",
    "words = [\"running\", \"flies\", \"easily\", \"fairly\", \"studies\"]\n",
    "\n",
    "stemmed_words = [regexp_stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e2c9e-0b9b-4d68-8efa-ca02af119bdc",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19aed9e7-183f-4c77-8068-9bf77e02c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'fly', 'better', 'foot']\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"flies\", \"better\", \"feet\"]\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)\n",
    "\n",
    "# We can also specify the pos\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77546330-b175-4762-ba32-a9139fb29238",
   "metadata": {},
   "source": [
    "### POS(Parts-of-Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85455d57-8e8a-49d0-9e3a-53b098f5e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Pos_tag\n",
    "from nltk import pos_tag\n",
    "\n",
    "sen = \"The quick brown fox jumps over the lazy dog.\"\n",
    "words = word_tokenize(sen)\n",
    "\n",
    "pos_tags = pos_tag(words)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2f20641-8fe5-4b4a-ab23-d1eb8a96e6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -> DET DT\n",
      "quick -> ADJ JJ\n",
      "brown -> ADJ JJ\n",
      "fox -> NOUN NN\n",
      "jumps -> VERB VBZ\n",
      "over -> ADP IN\n",
      "the -> DET DT\n",
      "lazy -> ADJ JJ\n",
      "dog -> NOUN NN\n",
      ". -> PUNCT .\n"
     ]
    }
   ],
   "source": [
    "#using spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sen = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(sen)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,\"->\", token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74a3e4-956b-4826-acaa-6f46d0abd3e6",
   "metadata": {},
   "source": [
    "### Named Entity Recognition \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e0f96d5-191d-4986-836c-16f30607ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bal Ram Reddy ->> PERSON\n",
      "India ->> GPE\n",
      "2002 ->> DATE\n"
     ]
    }
   ],
   "source": [
    "# Using spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sen = \"Bal Ram Reddy was born in India in 2002\"\n",
    "doc = nlp(sen)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\"->>\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b868200-8a23-456b-b8e9-b28673c8e189",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "153d775d-9fd6-48f8-8177-99449fa03f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: The, Dependency: det, Head: cat\n",
      "Word: cat, Dependency: nsubj, Head: sat\n",
      "Word: sat, Dependency: ROOT, Head: sat\n",
      "Word: on, Dependency: prep, Head: sat\n",
      "Word: the, Dependency: det, Head: mat\n",
      "Word: mat, Dependency: pobj, Head: on\n",
      "Word: ., Dependency: punct, Head: sat\n"
     ]
    }
   ],
   "source": [
    "sen = \"The cat sat on the mat.\"\n",
    "doc = nlp(sen)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"Word: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d0d1e-44de-409e-8328-178b5e025083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
